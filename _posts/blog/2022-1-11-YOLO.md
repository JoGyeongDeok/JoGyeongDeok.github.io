---
title: "YOLO"
category: DeepLearning
tags: [Computer Vision, Object Detection, Deep Learning]
comments: true
date : 2022-01-11
categories: 
  - blog
excerpt: CV
layout: jupyter
search: true
# 목차
toc: true  
toc_sticky: true 
use_math: true
---
<br>

***Yolo***

물체 인식(Object Detection)을 수행하기 위해 고안된 심층 신경망으로서, 테두리상자 조절(Bounding Box Coordinate)과 분류(Classification)를 동일 신경망 구조를 통해 동시에 실행하는 통합인식을 구현하는 것이 가장 큰 특징이다. 

| 항목 | V1 | V2 | V3 | 
|---|---|---|---| 
| 원본 이미지 크기  | 446 X 446 | 416 X 416 | 416 X 416 |
| Feature Extractor | Inception 변형 (VGG 기반) | Darknet 19 (BFLOPS 줄임과 동시에 수행성능 향상에 노력)| Darknet 53 (ResNet에 많은 영향받음)|
| Grid당 Anchor Box 수 | 2개(anchor box는 아님) | 5개 | Output Feature Map당 3개 서로 다른 크기와 스케일로 총 9개 |
|Anchor box 결정 방법  |  | K-Means Clustering | K-Means Clustering |
| Output Feature Map 크기 (Deep 제외) | 7 X 7 | 13 X 13 | 13 X 13, 26 X 26, 52 X 52 개의 Feature Map 사용|
| Feature Map Scaling 기법 |  |  | FPN (Feature Pyramid Network) |

**Anchor box**
- 고정된 사이즈와 크기를 가지고 있어야 한다.
- V1은 Anchor Box가 아니다.

**K-Means Clustering**
- 이미지 별 오브젝트의 크기, 형태등을 통해 Anchor Box 사이즈 결정

<br>

# YOLO Version1

- Yolo V1은 입력 이미지를 S X S Grid로 나누고 **각 Grid의 Cell이 하나의 Object에 대한 Detection 수행**
- 각 Grid Cell이 2개의 Bounding Box 후보를 기반으로 Object의 Bounding Box를 예측

7 X 7 Grid
> 총 Bounding Box 는 7 X 7 X 2 = 98


<img src = 'https://drive.google.com/uc?id=14GoYrzSHJBXl4psRcm9A340cKLA_hh6f' height = 500 width = 700>

"B"는 그리드 별로 갖는 Bounding Box의 후보 수
"C"는 Class의 개수

1. 이미지를 S X S의 그리드로 분할한다.
2. 이미지 전체를 신경망에 넣고 특징 추출을 통해 예측 텐서(Prediction Tensor)을 생성한다.
> 여기서 예측 텐서는 그리드별 테두리상자 정보, 신뢰 점수, 분류 클래스 확률을 포함한다.
3. 그리드 별 예측 정보를 바탕으로 테두리 상자 조정 및 분류 작업을 수행한다.
4. 각각의 Grid cell은 B개의 Bounding Box와 각 Bounding box에 대한 Confidence Score를 가진다.
> **Confidence Score : $Pr(Object)\ *\ IoU $**
5. 각각의 Grid cell은 C개의 Conditional Class Probability를 가진다.
6. 각각의 Bounding box는 x, y좌표, w, h, confidence를 지닌다.
> (x,y) : Bounding box의 중심점을 의미한다.

***YOLO - V1 Loss***

$$Loss\ =\ \lambda_{ccord}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\upharpoonleft_{ij}^{obj}[\ \ (x_i\ -\ \hat{x}_i\ )^2\ +\ (y\ -\ \hat{y}_i)^2\ ] \\ \color{orange}{
 \\ BBOX\ 중심\ x,\ y좌표\ Loss\\   
[예측\ 좌표x,\ y값과\ Ground\ Truth\ 좌표\ x,\ y값의\ 오차\ 제곱을\ 기반
\\ 모든\ Cell의\ 2개의\ Bbox(98개\ Bbox)\ 중에\ 예측\ Bbox를\ 책임지는\ Bbox만\ Loss\ 계산
\\ \upharpoonleft_{ij}^{obj}\ :\ \ 98개의\ Bbox\ 중\ 오브젝트\ 예측을 책임지는\ Bbox만\ 1\ 나머지는\ 0\ ]}
\\
+\ \lambda_{ccord}\sum_{i=0}^{S^2}\sum_{j=0}^{B} \upharpoonleft_{ij}^{obj}[\ (\sqrt{w_i}\ -\ \sqrt{\hat{w}_i}\ )^2\ +\ (\sqrt{h_i}\ -\ \sqrt{\hat{h}_i}\ )^2\ ]
\\ 
\color{orange}{BBox\ 너비\ w,\ 높이\ h\ Loss
\\ [BBOX\ 중심\ x,\ y좌표\ Loss
\ 예측\ 너비,\ 높이값과\ Ground\ Truth\ 너비,\ 높이\ 값의\ 오차\ 제곱을\ 기반으로\ 하되,\\ 크기가\ 큰\ 오브젝트의\ 경우\ 오류가\ 상대적으로\ 커짐을\ 제약하기\ 위해서\ 제곱근을\ 취함. \ ]}
\\
+\sum_{i=0}^{S^2}\sum_{j=0}^{B}\upharpoonleft_{ij}^{obj}(C_i\ -\ \hat{C}_i)^2\
+\ \lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\upharpoonleft_{ij}^{obj}(C_i\ -\ \hat{C}_i)^2
\\ 
\color{orange}{
Object\ Confidence\ Loss
\\
예측된\ Object\ Confidence\ Score와\ Ground\ Truth의\ IoU의\ 예측\ 오차를\ 기반\\ 
object를\ 책임지는\ Bbox\ confidence\ loss\ +\ Object가\ 없어야\ 하는\ Bbox의\ confidence\ loss
}
\\ 
+\sum_{i=0}^{S^2}\upharpoonleft_{i}^{obj}\sum_{c\ \in\ classes}(p_i(c)\ -\ \hat{p_i}(c))^2
\\ 
\color{orange}{
Classfication\ Loss
\\
예측\ classification\ 확률\ 오차의\ 제곱.\ Object를\ 책임지는\ Bbox만\ 대상
}$$

<img src = 'https://drive.google.com/uc?id=11lawHFJqqTJMiDi6yQbroGrZAtlK0bhF' height = 500 width = 650>

최대한 많은 Bounding Box를 생성 후 NMS를 통해 최종 Bbox 예측


YOLO - V1의 이슈
- Detection 시간은 빠르나 Detection 성능이 떨어진다. 특히 작은 Object에 대해 성능이 나쁘다.

<br>

# YOLO Version2

- Batch Normalization
- High Resolution Classifier : 네트웍의 Classifier 단을 보다 높은 resolution(448 X 448)로 fine tuning
- **13 X 13 feature map 기반에서 개별 Grid cell 별 5개의 Anchor box에서 Object Detection**
> anchor box의 크기와 ratio는 K-Means Clustering으로 설정.
- 예측 Bbox의 x,y 좌표가 중심 Cell 내에서 벗어나지 않도록 Direct Location Prediction 적용
- Darknet-19 Classification 모델 채택
- Classificaion layer를 fully Connected layer에서

 **Fully Convolution** 으로 변경하고 서로 다른 크기의 image들로 네트워크 학습

$$
\color{orange}{CNN \rightarrow B.N \rightarrow ReLU \rightarrow Dense}\\ 
BackBone(Feature Extraction) 과 Head(Conv)\\ 
BackBone은\ freezing,\ Head\ 부분만\ Weight\ Update
$$

---

***Anchor Box로 1Cell에서 여러 개 Object Detection***

SSD와 마찬가지로 1개의 Cell에서 여러 개의 Anchor를 통해 개별 Cell에서 여러 개 Object Detection 가능

K-Means Clustering을 통해 데이터 세트의 이미지 크기와 Shape Ratio 따른 5개의 군집화 분류를 하여 Anchor Box를 계산

<img src = 'https://drive.google.com/uc?id=1aepj68PyFQMCtZDkAYZ7rXSMrFGj2LDf' height = 500 width = 600>

***Output Feature Map***

<img src = 'https://drive.google.com/uc?id=19yS-e5izVT5HympAacAxwRTU5IyIY5In' height = 500 width = 600>

13 X 13 Grid와 5개의 Anchor Box

***Direct Location Prediction***



$
b_x\ =\ \sigma(t_x)\ +\ c_x\\
b_y\ =\ \sigma(t_y)\ +\ c_y\\
b_w\ =\ p_we^{t_x}\\
b_h\ =\ p_he^{t_h}\\
Pr(object)\cdot IoU(b,\ object)\ =\ \sigma(t_o)\\
\sigma\ :\ sigmoid\ function
$

<img src = 'https://drive.google.com/uc?id=1vfXSsyFtPEri3zXHb3kyIwO2m9Msn8mi' height = 500 width = 600>

- (pw, ph) : anchor box size
- (tx, ty, tw, th) : 모델 예측 offset 값
- (bx, by), (bw, bh) : 예측 Bounding box 중심 좌표와 Size

**Center 좌표가 Cell 중심을 너무 벗어나지 못하도록 0 ~ 1 사이의 시그모이드 값으로 조절**


***YOLO - V2 Loss***
- 논문에서는 YOLO V2 Loss에 대한 별도 언급이 없다
- YOLO V1 Loss와 유사한 Loss 식

$$Loss\ =\ \lambda_{ccord}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\upharpoonleft_{ij}^{obj}[\ \ (x_i\ -\ \hat{x}_i\ )^2\ +\ (y\ -\ \hat{y}_i)^2\ ] \\ \color{orange}{
 \\ BBOX\ 중심\ x,\ y좌표\ Loss\\}
\\
+\ \lambda_{ccord}\sum_{i=0}^{S^2}\sum_{j=0}^{B} \upharpoonleft_{ij}^{obj}[\ (\sqrt{w_i}\ -\ \sqrt{\hat{w}_i}\ )^2\ +\ (\sqrt{h_i}\ -\ \sqrt{\hat{h}_i}\ )^2\ ]
\\ 
\color{orange}{BBox\ 너비\ w,\ 높이\ h\ Loss}
\\
+\sum_{i=0}^{S^2}\sum_{j=0}^{B}\upharpoonleft_{ij}^{obj}(C_i\ -\ \hat{C}_i)^2\
+\ \lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\upharpoonleft_{ij}^{obj}(C_i\ -\ \hat{C}_i)^2
\\ 
\color{orange}{
Object\ Confidence\ Loss}
\\ 
+\sum_{i=0}^{S^2}\upharpoonleft_{i}^{obj}\sum_{c\ \in\ classes}(p_i(c)\ -\ \hat{p_i}(c))^2
\\ 
\color{orange}{
Classfication\ Loss}$$

---

***Passthrough module을 통한 fine grained feature***
- 좀 더 작은 오브젝트를 Detect 하기 위해서 26 X 26 X 512 feature map의 특징을 유지한 채 13 X 13 X 2048 로 reshape한 뒤 13 X 13 X 1024에 추가하여 feature map 생성

<img src = 'https://drive.google.com/uc?id=1ZfsQLl8lV0rmlKe5BSYuUtkQtXIqmP4n' height = 500 width = 600>

***Multi-Scale Training***
- Classification Layer가 Convolution Layer로 생성하여 동적으로 입력 이미지 크기 변경 가능
- 학습 시 10회 배치 시마다 입력 이미지 크기를 모델에서 320부터 608까지 동적으로 변경(32의 배수로 설정)

***Darknet 19 Backbone***
- VGG-16 : 30.69 BFlops, Top 5 Accuracy 90%
- Yolo v1 : 8.52 BFlops, Top 5 Accuracy 88%
- Darknet-19 : 5.58 BFlops, Top 5 Accuracy 91.2%

Classification Layer에 Fully Connected Layer를 제거하고 Conv Layer를 적용

---

<br>

# YOLO Version3

- FPN(Feature Pyramid Network) 유사한 기법을 적용하여 3개의 Feature Map Output에서 각각 3개의 서로 다른 크기와 scale을 가진 anchor box로 Detection
- Backbone 성능 향상 (Darknet - 53)
- Multi Labels 예측 : Softmax가 아닌 Sigmoid 기반의 logistic classifier로 개별 Object의 Multi labels 예측

***FPN(Feature Pyramid Network)***

<img src = 'https://drive.google.com/uc?id=1qp3JHlNRit1br4d2JGkNk9cmyteT-sc4' height = 500 width = 600>

- 상위에서 학습된 Feature + 하위에서 학습된 Feature
> 추상적인 부분 + 상세한 부분

***YOLO V3 모델 아키텍처***

<img src = 'https://drive.google.com/uc?id=1OaKrcr8rKWNysOQemiYq6mezOJZOzDjN' height = 500 width = 600>
<img src = 'https://drive.google.com/uc?id=1p2E1c2yOWcEgw8gT8GH1DtgxkLA977TY' height = 500 width = 600>

***Multi Labels***

여러 개의 독립적인 Logistic Classifier 사용

<img src = 'https://drive.google.com/uc?id=1rtw9tsNO1gAS4V9JshvgohoyJYL9y-UY' height = 500 width = 600>