---
title: "Multi Layer Perceptron(MLP)"
tags: [Pytorch, MLP, Deep Learning]
comments: true
date : 2022-01-02
categories: 
  - blog
excerpt: MLP
layout: jupyter
search: true
# 목차
toc: true  
toc_sticky: true 
use_math: true
---

# Neural Network

## 인공 뉴런

- 인공신경망은 서로 연결된 수 많은 인공 뉴런으로 구성되어 있으며 특저한 네트워크 구조를 따른다.
- 아래 그림의 네트워크 구조는 acyclic feed-forward network으로 입력층(Input layer), 은닉층(hidden layer), 출력층(output layer) 세 개의 층을 지니며, 각 마디간의 연결에는 가중치(weight)가 존재한다.
- 인공 신경망의 학습 방법들은 ***총 예측오차(total prediction error)***를 최소화할 수 있도록 은닉 마디들의 가중치를 조절하는 것이다.

<img src = 'https://drive.google.com/uc?id=1-ROfXJxT5t9dIPz4OqhChESSa4MaGpF2' height = 400 width = 400>

---

## 퍼셉트론(perceptron)

- **feed-forward network 모형**의 가장 간단한 형태인 **선형분류 모형**
- $∑_{i=0}^{∞}wᵢxᵢ$ 를 계산하고 계산된 값의 크기가 기준(threshold)보다 크면 1, 작으면 -1을 결과로 내보내는 함수이다.

- 무작위로 결정된 가중치로 시작하여, 학습 데이터에 대해 퍼셉트론을 적용하여 잘못 분류된 사례가 발생했을 때 가중치를 개선함
- 아래 가중치 학습 규칙은 퍼셉트론이 모든 학습 데이터를 정확하게 분류할 때까지 오랜 시간 동안 반복됨
- 퍼셉트론은 선형 모델일 시 완전히 분리 될 수 있지만, 비선형 모델일 시 완전히 분류할 수 없는 한계를 가지고 있다.


<img src = 'https://drive.google.com/uc?id=1Ff8FTI2awHgUZcVwEUjQYfBWgB7Wrk2C' height = 600 width = 700>

<img src = 'https://drive.google.com/uc?id=1kZCXUnxw2n2aFfjmVZ8-BO2WIzzu4ve1' height = 600 width = 700>

<img src = 'https://drive.google.com/uc?id=1_Jg8bohy4DRFRHZBECjS6UQ1j3YN9efM' height = 600 width = 700>

___

## Multi Layer Perceptron(MLP)

Input Layer의 Dimension은 3, Output Layer의 Dimension은 2

4개의 Hidden node로 이루어진 1개의 Hidden Layer로 이루어져 있다.

뉴럴 네트워크는 직선을 긋는 퍼셉트론의 조합으로 이루어져 있다.

<img src = 'https://drive.google.com/uc?id=167TxckBJ7Thw8vFicmQWEDJ93EAUqtJO' height = 1000 width = 700>

- 여러 perceptron의 조합으로 이루어진 network
- **입력 층, 출력 층과 함께 여러 개의 은닉 층으로 구성된 신경망 모형**

- **Feed forward**
 > * Neural network에 input이 들어와 output이 나오기까지의 과정
 > * Input -> Hidden -> output

- **Back Propagation**
 > * Neural network의 output과 error를 계산하여 neural network의 weight를 update
 > * Input -> Hidden -> ***output -> error -> hidden(back propagation)***



<img src = 'https://drive.google.com/uc?id=1Bzx9v-15DB-CJw9DIZLx5f9zPca7cBjA' height = 400 width = 400>

- Neural network는 feed forward / back propagation 모두 행렬로 연산됨.

- 활성함수(Input X Weight1) = Hidden 
>  활성함수에 input X Weight1을 대입
- Hidden X Weight2 = output
> 활성함수가 있을 수 있고 없을 수도 있다. (Linear Regression에선 필요 없다.)

### 활성 함수

<img src = 'https://drive.google.com/uc?id=1eo6YGap8TEyV5fdebwxxcBWKEbriGg4f' height = 600 width = 500>

- 입력 값과 가중치 곲의 합을 입력으로 받아 출력하는 함수로, 일반적으로 ***미분 가능***하고 단조 증가하는 시그모이드 함수를 기본적으로 사용
- 시그모이드 함수는 입력 값을 통해 비선형적인 출력 값을 얻고 싶을 때 사용 
> (선형 함수만을 사용할 경우, 여러 층의 선형 함수들을 계층적으로 연결하더라도 선형적인 출력만이 가능)

$$
\sigma(x) = \frac{1}{1+e^{-x}} \\ and\\ \frac{d\sigma(x)}{dx} =\sigma(x)(1-\sigma(x))
$$



### 경사 하강법(Gradient descent method)

<img src = 'https://drive.google.com/uc?id=1ueWTgNMxTD-ZubV6pn2tUkStJjOlMD8A' height = 600 width = 500>

- 과적합 문제 발생


### Feed Forward

<img src = 'https://drive.google.com/uc?id=1b70phJ_ELZfk_viAXhRmL2b2KFPOZS9U' height = 400 width = 400>

- 시그모이드 함수 σ(x)
- Hidden Layer "$Z_h$"
- "i"는 Input 노드의 수 ,"j"는 Output 노드의 수
- "k"는 Feed, Back 한번이 1 epoch이다.

- Input -> Hidden
$$Z_h^k\ =\ ∑ w_{ih}^kx_i^k \\ \sigma(Z_h^k)\ =\ \frac{1}{1+e^{-Z_h^k}}$$

- Hidden -> Output
$$y_j^k\ =\ \sum w_{hj}^k\sigma(Z_h^k)\\ \sigma(y_j^k)\ =\ \frac{1}{1+e^{-y_j^k}}$$


---

- "t"는 실제 값
- $"σ(y_j^k))^2"는 예측값$

$$w_{ih}^{k+1}\ =\ w_{ih}^k\ +\ \triangle w_{ih}^k\ ,\ \epsilon_k\ =\ \frac{1}{2}\ \sum(\ t_j^k\ -\ \sigma(y_j^k)\ )^2 
\\ \ \ \ \ \ \ \ \ =\ w_{ih}^k\ -\ \frac{\partial\ \epsilon_k}{\partial w_{ih}^k} 
\\ w_{hj}^{k+1}\ =\ w_{hj}^k\ +\ Δw_{hj}^k\  
\\ \ \ \ \ \ \ \ \ \ \ =\ w_{hj}^k\ -\ \frac{∂\ \partial_k}{\partial w_{hj}^k}$$

### Back propagation (역전파 알고리즘)

<img src = 'https://drive.google.com/uc?id=14GT-d1UNNq0inmgCJD3E9BAiUq7aYqS-' height = 1000 width = 700>

***Weight2***

1. $$ w_{hj}^{k+1} 부터 학습$$

2. $$w_{hj}^{k+1}\ =\ w_{hj}^k\ +\ Δw_{hj}^k\ =\ w_{hj}^k\ -\ \frac{\partial \ \epsilon_k}{\partial w_{hj}^k}$$

3. $$Z_h^k\ =\ \sum_{ih}^kx_i^k\ \ , \ \ \ \ \ \ \ \ \ \ \ \ \ \  \sigma(Z_h^k)\ =\ \frac{1}{1+e^{-Z_h^k}}\\ y_j^k\ =\ \sum w_{hj}^k\sigma(Z_h^k)\ \ , \ \ \ \ \ \ \sigma(y_j^k)\ =\ \frac{1}{1+e^{-y_j^k}}\\ \epsilon_k\ =\ \frac{1}{2}\sum(\ t_j^k\ -\ \sigma(y_j^k)\ )^2$$



- Weight2는 에러, 최종 y의 예측값, 이전 Layer 값으로 결정됨
- Loss ε 대신 Cross Entropy 등 다른 에러함수 사용 가능
- 시그모이드 활성함수 대신 Tangent 등 다른 활성함수 사용 가능
- Chain rule 사용

***Weight1***

- $$w_{ih}^{k+1}\ =\ w_{ih}^k\ +\ \triangle w_{ih}^k\ =\ w_{ih}^k\ -\ \frac{\partial\ \epsilon_k}{\partial w_{ih}^k}$$


<img src = 'https://drive.google.com/uc?id=14UnrZ-SBZ15P203APQoxQXtu_NP7P5MT' height = 1000 width = 700>

<img src = 'https://drive.google.com/uc?id=1ya9smgO2n4xvuLS_lXlfR7LhC7eyyoQd' height = 1000 width = 700>

<br>

# 신경망 모형의 단점

- 역전파 알고리즘은 아직 인식하지 못한 데이터보다는 학습데이터에 지나치게 과적합할 가능성이 높음
- Gradient vanishing 현상
> * $$\triangle w_{hj}^k \ \ , \ \triangle w_{hj}^k$$ 가 점점 작아짐 
> * Hidden Layer를 많이 쌓을수록 Input Layer에 가까운 Layer일 수록 Gradient vanishing 현상이 매우 심해짐<br> => 학습이 안될 가능성이 커진다.
> * 복잡한 문제일 수록 Hidden Layer를 쌓아야 하는데 학습이 안될 수도 있다.
